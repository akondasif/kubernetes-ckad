-- â˜…â˜… Docker Kubernetes Commands â˜…â˜… --

legend
â˜…  : new big topic
-- : smaller topics
#  : command
=  : command execution result 


â˜…â˜…â˜…Creating, Running, and Sharing Container Imageâ˜…â˜…â˜…
--Running simple container 
# docker run <image>:<tag>
ex:
# docker run busybox:latest echo "Hello World" 
  = docker pull busybox-image from Docker Registry or local machine, create a container and run the "Hello World"

--Creating a Dockerfile for the image
Dockerfile sample: 
 FROM node:7
 ADD app.js /app.js
 ENTRYPOINT ["node", "app.js"]

Directory tree: 
.
â”œâ”€â”€ Dockerfile
â””â”€â”€ app.js

--Build container image
# docker build [OPTIONS] PATH
# docker build -t kubia .
-t, --tag list : Name and optionally a tag in the 'name:tag' format

--Check the built images
# docker images ls
# docker images ls | grep <image that had just been build>
  = REPOSITORY			TAG			IMAGE ID		CREATED			SIZE
    kubia				latest		01741bed2b6c	2 weeks ago		660MB
    luksa/kubia			latest		bf5bc565dc79	2 weeks ago		660MB

--Remove built image
# docker image rm <image-name>
# docker image rm -f <image-name>

--Running the container image
# docker run --name <container-name> -p <port> -d <image-name-to-be-detached>
# docker run --name kubia-container -p 8080:8080 -d kubia
-p, --publish list : Publish a container's port(s) to the host
-d, --detach : Run container in background and print container ID

--Check whether the container normally built
# docker container ls
# docker ps -a

--Accessing http docker app
# curl localhost:8080
or open your browser and access localhost:8080

--Listing all running containers
# docker ps 
CONTAINER ID 	IMAGE 	COMMAND 		CREATED 		STATUS 			PORTS                    NAMES
5844d8fa5c50 	kubia 	"node app.js" 	18 minutes ago 	Up 18 minutes 	0.0.0.0:8080->8080/tcp   kubia-container

--List all running and stopped containers
# docker ps -a
CONTAINER ID		IMAGE			COMMAND			CREATED			STATUS					PORTS	NAMES
796f0b2a7fca		luksa/kubia		"node app.js"	2 weeks ago		Exited	(137)2 weeks ago		ff-kubia-container
ca08efc6669e		kubia			"node app.js"	2 weeks ago		Exited (137) 2 weeks ago		kubia-container

--Getting additional information about a container
# docker inspect [OPTIONS] NAME|ID [NAME|ID...]
# docker inspect kubia-container
Docker will print out a long JSON containing low-level information about the container. 

--Exploring the inside of running container
# docker exec -it <container-name> bash
# docker exec -it kubia-container bash
-i: makes sure STDIN is kept open
-t: allocates pseudo terminal (TTY)

--Stopping and Removing Container
# docker stop <container-name|id>   --> # docker start <conainer-name|id>
# docker rm <container-name|id>


--Pushing the image to an image registry
--Login to http://hub.docker.com
# docker login
Username: 		#famy
Password: 
Login succeeded

--Tag image under additional tag
# docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
# docker tag kubia famy/kubia
# docker image ls | head
REPOSITORY		TAG			IMAGE ID		CREATED			SIZE
famy/kubia		latest		bf5bc565dc79	2 hours ago		660MB
kubia			latest		bf5bc565dc79	2 hours ago		660MB

--Pushing the image to Docker Hub
# docker push famy/kubia
!! make sure your docker hub id same as the tag name

--Running the image on different machine
# docker run -p 8080:8080 -d famy/kubia



â˜…â˜…Kubernetes with Minikube
--Installing Minikube
MAC OSX
https://kubernetes.io/docs/tasks/tools/install-minikube/
# brew cask install minikube

--Starting Minikube
# minikube start
# minikube status
  host: Running
  kubelet: Running
  apiserver: Running
  kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.112

--Check cluster availability
# kubectl cluster-info

--Deploying Node.js app to Kubernetes
# kubectl run kubia --image=famy/kubia --port=8080 --generator=run/v1
# kubect get pods

--Accessing your web application pod
To make pod accessible from outside, you'll expose it through a Service object. 
A special service of type LoadBalancer need to be configured not only enable internal access but also provide public IP to the pod. 

-Expose the ReplicationController (rc)
# kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   1         1         1       16h
# kubectl expose rc kubia --type=LoadBalancer --name kubia-http
  =  service/kubia-http exposed

-Listing services
# kubectl get services
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          17h
kubia-http   LoadBalancer   10.108.140.71   <pending>     8080:30188/TCP   3m33s
!! Minikube doesn't support LoadBalancer, so the external IP will not available. But we may access the pod via external ports. 

-Run and get the IP and port (Minikube)
# minikube service <service-name>
# minikube service kubia-http
ðŸŽ‰  Opening kubernetes service default/kubia-http in default browser...

-Replication Controller Role
It makes sure there is always exactly one instance of your pod running. If a pod were disappear for any reason, the Replication Controller will create a new pod to replace the missing one. 
The number of instance can be configured in ReplicationController

--Scaling application
# kubectl get rc
# kubectl scale rc <rc-name> --replicas=<number-of-replicas>
# kubectl scale rc kubia --replicas=3

--Minikube dashboard
# kubectl cluster-info 
# minikube dashboard
 


â˜…â˜…â˜…Chap3 PODS: running containers in Kubernetesâ˜…â˜…â˜…
--Flat inter-pod network
All pods in Kubernetes cluster reside in a single flat, shared, network-address space, which means every pod can access every other pod at other pod's IP address. 
No NAT (Network Address Translation) gateway exist between them, and pods can communicate with each other like computers on local area network (LAN). 

--Creating pods from YAML or JSON descriptor
--Examining YAML descriptor of a POD
# kubectl get pod <pod-name> -o yaml 
# kubectl get pod <pod-name> -o json

--A simple YAML descriptor for a pod 
apiVersion: ver1			#1
kind: Pod 					#2
metadata:
  name: kubia 				#3
spec: 
  containers: 
  - image: luksa/kubia 		#4
    name: kubia 			#5
    ports: 
    - containerPort: 8080 	#6
      protocol: TCP

#1 Descriptor conforms to version v1 of Kubernetes API
#2 Youâ€™re describing a pod.
#3 The name of the pod
#4 Container image to create the container from
#5 Name of the container
#6 The port the app is listening on

--Pod detail attribute
# kubectl explain pods
# kubectl explain pod.spec
# kubectl explain pod.metadata

--Using kubectl create to create the pod
# kubectl create -f <yaml-file>
  = "kubectl create -f" command is used to create any resource from YAML or JSON

--Viewing application logs 
# kubectl logs <pod-name>
# kubectl logs <pod-name> -c <container-name>
-Container logs are automatically rotated daily and every time the log file reaches 10MB in size. 
-The kubectl logs command only shows the log entries from the last rotation. 

--Forwarding a local network port to a port in the pod
-When you want to talk to a specific pod without going through a service, Kubernetes allows you to configure port forwarding to the pod
# kubectl port-forward <pod-name> <from-port>:<to-port>


â˜…ORGANIZING PODS WITH LABELSâ˜…
With microservices architecture, the number of pods can easily exceed 20 or more. Those components may be replicated and multipled accordingly by its release (stable, beta, canary, etc) and also will run concurrently. This lead to hundreds of Pods in the system. We need a way of organizing pods into smaller groups based on arbitrary criteria, so the developer and administrator can easily see which pod is which. 

Organizing pods and all other Kubernetes objects is done through LABELs.
A label is an arbitrary key-value pair attached to a resource, and utilized when selecting resources using 'label selector'. 
-app, spesifies which app, component, or microservice the pod belongs to. 
-rel, shows whether the application running in the pod is a stable, beta, or canary release. 
â€»Canary release = when you deploy a new version of an application next to the stable version, but only a parts of users can access the new version (to see how it behaves before rolling it out to all users).

--Specify label to a pod
apiVersion: ver1
kind: Pod
metadata:
  name: kubia-manual-v2
  labels: 
    creation_method: manual 	#1
    env: prod 					#1
spec: 
  containers:
  - image: famy/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP

#1 two labels attached to the pod
labelname = kubia-manual-v2.yaml

--Create pod with label
# kubectl create -f <pod-yaml>
# kubectl create -f kubia-manual-v2.yaml
  = pod "kubia-manual-v2" created

# kubectl get pod --show-labels
NAME              READY   STATUS    RESTARTS   AGE   LABELS
kubia-2q9hb       1/1     Running   3          17d   run=kubia
kubia-manual-v2   1/1     Running   0          10m   creation_method=manual,env=prod

--Add labels of existing pod
# kubectl label pod <pod-name> <labelname>=<label-value>
# kubectl label pod kubia-manual creation_method=manual

--Modify/overwrite labels 
# kubectl label pod <pod-name> <existing-label-name>=<new-value> --overwrite
# kubectl label pod kubia-manual creation_method=auto --overwrite

--List pod with labels
# kubectl get pod -L creation_method,app
NAME              READY   STATUS    RESTARTS   AGE    CREATION_METHOD   APP
kubia-2q9hb       1/1     Running   3          17d
kubia-manual      1/1     Running   0          84m                      test
kubia-manual-v2   1/1     Running   0          111m   manual
kubia-r7t7b       1/1     Running   1          16h

--List subset with 'label selector'
A label selector can list pod as:
-label with certain key only
-label with key and value

# kubectl get pod -l creation_method --show-labels
NAME              READY   STATUS    RESTARTS   AGE    LABELS
kubia-manual-v2   1/1     Running   0          118m   creation_method=manual,env=prod

# kubectl get pod -l creation_method=manual
NAME              READY   STATUS    RESTARTS   AGE
kubia-manual-v2   1/1     Running   0          116m

# kubectl get pod -l '!creation_method' --show-labels
NAME           READY   STATUS    RESTARTS   AGE   LABELS
kubia-2q9hb    1/1     Running   3          17d   run=kubia
kubia-manual   1/1     Running   0          91m   app=test,rel=test
kubia-r7t7b    1/1     Running   1          16h   run=kubia

# kubectl get pod -l app,rell
NAME           READY   STATUS    RESTARTS   AGE   LABELS
kubia-manual   1/1     Running   0          91m   app=test,rel=test


--Using labels for categorizing worker nodes
# kubectl get nodes --show-labels
# kubectl label node <node-name> <label-name>=<label-value>
# kubectl get nodes -l <label-name>=<label-value>


--Scheduling pods to specific nodes
apiVersion: ver1
kind: Pod
metadata:
  name: kubia-gpu
spec: 
  nodeSelector:				# 1
    gpu: "true" 			# 1
  containers: 
  - image: luksa/kubia
    name: kubia

# 1 = nodeSelector tells Kubernetes to deploy this pod only to a certain nodes labeled with gpu=true
! Don't forget to associates the nodes with label as mentioned before. 
  # kubectl label node <node-name> gpu=true

--Adding and modifying annotations
# kubectl annotation pod <pod-name> <annotation>=<value>
# kubectl annotation pod kubia-manual mycompany.com/someannotation="foo-bar"



â˜…USING NAMESPACES TO GROUP RESOURCESâ˜…
# kubectl get namespaces
NAME                   STATUS   AGE
default                Active   18d
kube-node-lease        Active   18d
kube-public            Active   18d
kube-system            Active   18d
kubernetes-dashboard   Active   18d

# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-5644d7b6d9-lxf7p           1/1     Running   3          18d
coredns-5644d7b6d9-rggml           1/1     Running   3          18d
etcd-minikube                      1/1     Running   3          18d

--Creating namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace

# kubectl create -f custom-namespace.yaml
-OR-
# kubectl create namespace custom-namespace

--Creating pods to specific namespaces
# kubectl create -f pods-yaml.yaml -n custom-namespace

!! When listing, describing, modifying or deleting objects in other namespaces, you need to pass the --namespace (or -n) flag to kubectl. If you dont, kubectl performs the action in the default namespace configures in the current kubectl context. 



â˜…STOPPING AND REMOVING PODSâ˜…
--Delete pod by name
# kubectl delete pod <pod-name>

--Delete pod by label selectors
# kubectl delete pod -l <label-name>=<value>
# kubectl delete pod -l creation_method=manual

--Delete pod and namespace
# kubectl delete namespace <namespace-name>
# kubectl delete ns custom-namespace

--Delete all pods in a namespace 
# kubectl delete pod --all

--Delete almost all resources in a namespace
# kubectl delete all --all



â˜…â˜…â˜…Chap4 REPLICATION AND OTHER CONTROLLERS: deploying managed podsâ˜…â˜…â˜…
--Keeping pods healthy
Kubernetes will restart pod automatically when an application has a bug causes it to crash. Kubernetes automatically gives it the ability to heal itself. 

--Liveness probes
a. HTTP GET probe
HTTP GET request performed on the container's IP, a port and path that specified in advance. If the HTTP response code is 2xx or 3xx the probe is considered successful. 
b. TCP Socket probe
Open a TCP connection to the specified port of the container. If the connection successfully established, the probe is success. 
c. Exec probe
Execute an arbitrary command inside the container and check the command's exit code. If the return code 0, the probe is success. 

!!For pods running in production, you should always define a liveness probe. 
!!Keeps probe light, liveness probes should not use too many computational resources and should not take too log to compute. 

Example yaml: 
apiVersion: v1
kind: Pod
metadata: 
  name: kubia-liveness
spec: 
  containers: 
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe: 
      httpGet:
        path: /
        port: 8080

--Obtaining logs from the crashed pods
# kubectl logs <pod-name> --previous

--Configuring additional properties of the liveness probe
spec: 
  containers: 
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe: 
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 15	#1
#1 : Kubernetes will wait 15 seconds before executing the first probe


â˜…REPLICATION CONTROLLERâ˜…
-Replication Controller is a Kubernetes resource that ensure its pods to keep running. If pod dissapear for any reason, the Replication Controller notices the missing pod and creates the replacement pod. 

-Three essential parts of ReplicationController: 
a. label selector, which determines what pods are in the RC scope
b. replica count, which specifies the desired number of pod should be running
c. pod template, which is used to when creating the pod

-Advantages using Replication Controller: 
a. It makes sure a pod is always running by starting a new pod when an existing one goes down
b. It creates a replacement replicas for all pods when a cluster node fails, and re-create the pods on the fail node
c. It enables easy horizontal scaling of pods

--Creating a ReplicationController
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080

--Moving pods in and out of the scope of a ReplicationController
A ReplicationController(RC) manages pods that match its label selector. By changing a pod's labels, it can be removed from or added to the scope of RC. 

# kubectl get rc 
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       12d

# kubectl describe rc kubia | egrep "Selector|Labels"
Selector:     app=kubia
Labels:       app=kubia
  Labels:  app=kubia

# kubectl get pods --show-labels
NAME              READY   STATUS    RESTARTS   AGE   LABELS
kubia-2qcwd       1/1     Running   1          12d   app=kubia
kubia-hs4ml       1/1     Running   1          12d   app=kubia
kubia-wzwxs       1/1     Running   1          12d   app=kubia
kubia-manual      1/1     Running   2          14d   app=test,rel=test
kubia-manual-v2   1/1     Running   2          15d   creation_method=manual,env=prod

# kubectl label pod kubia-wzwxs app=foo --overwrite
pod/kubia-wzwxs labeled

# kubectl get pods --show-labels
NAME              READY   STATUS    RESTARTS   AGE   LABELS
kubia-2qcwd       1/1     Running   1          12d   app=kubia
kubia-hs4ml       1/1     Running   1          12d   app=kubia
kubia-6qdc4       1/1     Running   0          20s   app=kubia      # newly created pod (controlled by RC)
kubia-manual      1/1     Running   2          14d   app=test,rel=test
kubia-manual-v2   1/1     Running   2          15d   creation_method=manual,env=prod
kubia-wzwxs       1/1     Running   1          12d   app=foo        # label-changed pod

--Delete a ReplicationController
When you delete a RC with kubectl delete, the corresponded pods are also deleted. 
# kubectl delete rc <RC-name>

To delete a RC only and keep its pods can be managed by this command: 
# kubectl delete rc <RC-name> --cascade=false


â˜…REPLICASETâ˜…
ReplicaSet(RS) is a new generation of ReplicationController. It works exactly like a RC, and has more expressive pod selectors. RS can match the pods label or the label-value. 

RS sample yaml(kubia-relicaset.yaml)
apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name: kubia-rs
spec: 
  replicas: 3
  selector: 
    matchLabels: 
      app: kubia
    template: 
      metadata: 
        labels: 
          app: kubia
      spec: 
        containers: 
        - name: kubia
          image: luksa/kubia

# kubectl create -f kubia-replicaset.yaml
# kubectl get pods --show-labels
NAME              READY   STATUS    RESTARTS   AGE     LABELS
kubia-rs-8lvxg    1/1     Running   0          4m51s   app=kubia
kubia-rs-jzbqk    1/1     Running   0          4m51s   app=kubia
kubia-rs-tkvqr    1/1     Running   0          4m51s   app=kubia


â˜…DAEMONSET -Running exactly one pod on each node-â˜…
Daemonset create exactly one pod on each node. 
When a new node is added to the cluster, the Daemonset immediately deploys a new pod instance to it. As well as when a pod is deleted from a node, Daemonset will make sure one pod is created on each node. 

--Using a DaemonSet to run pods only on certain nodes
A DaemonSet deploys pod on all nodes in the cluster, unless you specify that the pods should only run on a subset of all the nodes. This can be done by specifying the node-Selector property in the pod template. 

!!DaemonSet deploys pod into a node even that node is being made unschedulable (cordoned, preventing pods from being deployed to them). Since the unschedulable attribute is only used by the Scheduler, whereas pods managed by DaemonSet bypass the Scheduler completely.  

DaemonSet (ssd-monitor-daemonset.yaml)
apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: ssd-monitor
spec: 
  selector: 
    matchLabel: 
      app: ssd-monitor
    template: 
      metadata: 
        labels: 
          app: ssd-monitor
      spec: 
        nodeSelector: 
          disk: ssd
        containers: 
        - name: main
          image: luksa/ssd-monitor

# kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        11s
!!Desired number of DS is 0. Need to check the node-label, that matched the setting "disk=ssd"

# kubectl get nodes --show-labels
NAME       STATUS   ROLES    AGE   VERSION   LABELS
minikube   Ready    master   32d   v1.16.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,node-role.kubernetes.io/master=
!!"disk=ssd" is not available at the ready node. Need to add manually the label to the node. 

# kubectl label node minikube disk=ssd
node/minikube labeled

# kubectl get nodes --show-labels
NAME       STATUS   ROLES    AGE   VERSION   LABELS
minikube   Ready    master   32d   v1.16.0   disk=ssd,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,node-role.kubernetes.io/master=

# kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   1         1         1       1            1           disk=ssd        3m43s

# kubectl get pods -l app=ssd-monitor -o wide
NAME                READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
ssd-monitor-d8hsj   1/1     Running   0          2m32s   172.17.0.15   minikube   <none>           <none>


â˜…JOB, resource that perform a single completable taskâ˜…
Job resource allows you to run a pod whose container isn't restarted when the process running inside finishes successfully. Once it does, the pod is considered COMPLETE. 

--Defining Job resource (batch-job.yaml)
apiVersion: batch/v1
kind: Job
metadata: 
  name: batch-job
spec: 
  template: 
    metadata: 
      labels: 
        app: batch-job
    spec: 
      restartPolicy: OnFailure
      containers: 
      - name: main
        image: luksa/batch-job

# kubectl create -f batch-job.yaml
# kubectl get pods 
NAME                READY   STATUS    RESTARTS   AGE
batch-job-zbqd6     1/1     Running   0          13s

# kubectl logs batch-job-zbqd6
Mon Nov  4 05:50:39 UTC 2019 Batch job starting

!! The dockerfile for the above Job resource as follow: 
FROM busybox
ENTRYPOINT echo "$(date) Batch job starting"; sleep 120; echo "$(date) Finished succesfully"

After 120 seconds, the pod will finish its job and exit successfully: 

# kubectl get pods
NAME                READY   STATUS      RESTARTS   AGE
batch-job-zbqd6     0/1     Completed   0          3m23s

# kubectl logs batch-job-zbqd6
Mon Nov  4 05:50:39 UTC 2019 Batch job starting
Mon Nov  4 05:52:39 UTC 2019 Finished succesfully

--Running multiple pod instance in a Job
If you need a Job to run more than once, put the "completions" attribute to the Job yaml definition. 

apiVersion: batch/v1
kind: Job
metadata: 
  name: multi-completion-batch-job
spec: 
  completions: 5
  template: ...

--Running job pods in parallel
...
spec: 
  completions: 5
  parallelism: 2
...

--Scaling a Job
# kubectl scale <job-name> --replicas 3

--Limiting the time allowed for a Job to complete
You can configure how many times a Job can be retried before it is marked as failed, by specifying the spec.backoffLimit field in the manifest yaml file. The default is 6


â˜…CronJobâ˜…
CronJob allows you to schedule jobs to run periodically or once in the future. 
A cron job in Kubernetes is configured by creating a CronJob resource. 

apiVersion: batch/v1
kind: CronJob
metadata: 
  name: batch-job-every-five-minutes
spec: 
  schedule: "*/5 * * * *"
  jobTemplate: 
    spec: 
      template: 
        metadata: 
          labels: 
            app: periodic-batch-job
        spec: 
          restartPolicy: OnFailure
          containers: 
          - name: main
            image: luksa/batch-job

--Check the executed jobs
# kubectl get jobs
NAME                                      COMPLETIONS   DURATION   AGE
batch-job-every-five-minutes-1572854460   1/1           2m4s       3m12s
batch-job-every-five-minutes-1572854580   0/1           72s        72s



â˜…â˜…â˜…Chap5 SERVICES: enabling clients to discover and talk to podsâ˜…â˜…â˜…
In the case of microservices, pods will respond to HTTP requests either coming from other pods inside the cluster or from clients outside the cluster.
In the non-Kubernetes world, sysadmin will configure each client-app by specifying the exact IP or hostname of the server providing the service. But, doing the same in Kubernetes won't work, because: 
1. Pods are ephemeral
Pods may come and go at any time due to it is removed, scaled down, or because a cluster node failed. 
2. Kubernetes assigns an IP to a pod after the pod has been scheduled to a node and before it's started. 
3. Horizontal scaling means multiple pods may provide the same service. 
Client shouldnt care how may pods backing the service and what their IPs are. Instead, all those pods should be accessible through a single IP address. 

To solve all the above problems, Kubernetes provide Service resource type. 

â˜…INTRODUCING SERVICESâ˜…
-A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and PORT that never change while the service exist. 
-Label selectors determine which pods belong to the Service. 

Service sample yaml (kubia-svc.yaml)
apiVersion: v1
kind: Service
metadata: 
  name: kubia-svc
spec:
  ports: 
  - port: 80            #1
    targetPort: 8080    #2
  selector: 
    app: kubia          #3

#1 the port the service will be available on
#2 the container port the service will forward to 
#3 All pod with the "app=kubia" label will be part of this service

# kubectl create -f kubia-svc.yaml
# kubectl get svc 
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubia-svc    ClusterIP   10.97.237.121   <none>        80/TCP    4m26s

-From the above list, IP address 10.97.237.121 is assigned to the kubia-svc service. This is the cluster IP and only accessible from inside the cluster. The primary purpose of service is to expose group of pods to other pods in the cluster. But ofcourse we can expose the service outside the cluster too. 

-We can send requests to the above service by several ways: 
1. Create a pod that will send the service's cluster IP and log the response. 
2. ssh into one of the Kubernetes nodes and use the curl command. 
3. execute the curl command inside one of existing pods (through kubectl exec)

# kubectl get pods --show-labels
NAME              READY   STATUS    RESTARTS   AGE     LABELS
kubia-2qcwd       1/1     Running   1          12d     app=kubia
kubia-rs-8lvxg    1/1     Running   0          7h22m   app=kubia
kubia-wzwxs       1/1     Running   1          12d     app=foo

# kubectl exec -it kubia-wzwxs bash
  root@kubia-wzwxs:/# curl http://10.97.237.121:80
  You've hit kubia-2qcwd

  root@kubia-wzwxs:/# curl http://10.97.237.121
  You've hit kubia-rs-8lvxg

  root@kubia-wzwxs:/# exit

# kubectl exec kubia-wzwxs -- curl -s http://10.97.237.121:80
You've hit kubia-rs-8lvxg

!! The double dash (--) in the command signals the end of command options for kubectl. Everything after the double dash is the command that should be executed inside the pod.


â˜…CONNECTING TO SERVICES LIVING OUTSIDE THE CLUSTERâ˜…


â˜…EXPOSING SERVICES TO EXTERNAL CLUSTERâ˜…


â˜…EXPOSING SERVICES EXTERNALLY THROUGH AN INGRESS RESOURCEâ˜…


â˜…SIGNALING WHEN A POD IS READY TO ACCEPT CONNECTIONSâ˜…


â˜…USING A HEADLESS SERVICE FOR DISCOVERING INDIVIDUAL PODSâ˜…





â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
CKAD CRASH COURSE
â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

--Documentation: 
https://kubernetes.io/docs/reference/kubectl/cheatsheet/
https://matthewpalmer.net/kubernetes-app-developer/#purchase-the-ebook
https://github.com/dgkanatsios/CKAD-exercises

--Time Management: 
19 Problems in 2 hours. Use your time wisely!
There might be weight of point for specific questions. 

--Using Alias for kubectl 
# alias k=kubectl 
# k version 
Client Version: version.Info{Major:"1", Minor:"12", GitVersion:"v1.12.2", GitCommit:"17c77c7898218073f14c8d573582e8d2313dc740", GitTreeState:"clean", BuildDate:"2018-10-24T06:54:59Z", GoVersion:"go1.10.4", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.2", GitCommit:"c97fe5036ef3df2967d086711e6c0c405941e14b", GitTreeState:"clean", BuildDate:"2019-10-15T19:09:08Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}

--Setting Context & Namespace 
# kubectl config set-context <context-of-question> --namespace=<namespace-of-question>

--Deleting Kubernetes Objects 
Deleting object may take time, to save your time during the exam, don't wait for a graceful deletion of objects, just kill it!
# kubectl delete pod <pod-name> --grace-period=0 --force
useful flag: --grace-period=0 --force

--Understand and Practice bash 
# if [ ! -d ~/tmp ]; then mkdir -p ~/tmp; fi; while true; do echo $(date) >> ~/tmp/date.txt; sleep 5; done; 
# while true; do kubectl get pods | grep docker-regist; sleep 3; done; 

--Object Management
-Imperative method : kubernetes
> fast but requires detailed knowledge, no track record  
# kubectl create namespace ckad
# kubectl run nginx --image=nginx --restart=Never -n ckad 
-Declarative method: yaml
> Suitable for more elaborate changes, tracks changes 
-Hybrid approach
>Generate YAML file with kubectl
# kubectl run nginx --image=nginx --restart=Never --dry-run -o yaml > nginx-pod.yaml 
# vim nginx-pod.yaml 
# kubectl apply -f nginx-pod.yaml 

--Pod life cycle
-Pending
-Running
-Succeeded
-Failed 
-Unknown 

--Inspecting a Pod's status: 
-Get current status and event logs: 
# kubectl describe pods <pod-name> | grep Status:

-Get current lifecycle phase: 
# kubectl get pods <pod-name> -o yaml | grep phase 

--Configuring Env. Variables
Injecting runtime behaviour
apiVersion: v1 
kind: Pod 
metadata: 
  name: spring-boot-app 
spec: 
  containers: 
  - image: bmuchko/spring-boot-app:1.5.3
    name: spring-boot-app 
    env:                              # Added this 
    - name: SPRING_PROFILES_ACTIVE    # Added this 
      value: production               # Added this 

--Commands and Arguments
Running a command inside of container
apiVersion: v1 
kind: Pod 
metadata: 
  name: nginx 
spec: 
  containers: 
  - image: nginx:latest 
    name: nginx 
    args:                 # Added this 
    - /bin/sh             # Added this 
    - -c                  # Added this 
    - echo hello world    # Added this 

--Other Useful kubectl commands 
# kubectl logs <pod-name> 
# kubectl exec -it <pod-name> -- /bin/sh 


# Exercise 1
In this exercise, you will practice the creation of a new Pod in a namespace. Once created, you will inspect it, shell into it and run some operations inside of the container.
## Creating a Pod and Inspecting it
1. Create the namespace `ckad-prep`.
2. In the namespace `ckad-prep` create a new Pod named `mypod` with the image `nginx:2.3.5`. Expose the port 80.
3. Identify the issue with creating the container. Write down the root cause of issue in a file named `pod-error.txt`.
4. Change the image of the Pod to `nginx:1.15.12`.
5. List the Pod and ensure that the container is running.
6. Log into the container and run the `ls` command. Write down the output. Log out of the container.
7. Retrieve the IP address of the Pod `mypod`.
8. Run a temporary Pod using the image `busybox`, shell into it and run a `wget` command against the `nginx` Pod using port 80.
9. Render the logs of Pod `mypod`.
10. Delete the Pod and the namespace.

# Solution 1 
# kubectl create namespace ckad-prep 
# kubectl get ns | grep ckad-prep 
# kubectl run mypod --image=nginx:2.3.5 --port=80 --namespace=ckad-prep --restart=Never
# kubectl describe pod mypod -n ckad-prep | tee pod-error.txt 
â†’ Error happened since the image can't be pulled (specified tag of the image doesn't exist)
# kubectl edit pod mypod -n ckad-prep 
â†’ change this line to 
  image: nginx:1.15.12
# kubectl get pod -n ckad-prep 
# kubectl exec -it mypod -n ckad-prep -- /bin/sh 
  # ls (inside the pod)
  # exit 
# kubectl get pods -n ckad-prep -o wide | grep mypod 
mypod   1/1     Running   0          93m   172.17.0.14   minikube   <none>           <none>
# kubectl run busybox --image=busybox --rm -it --restart=Never --namespace=ckad-prep -- /bin/sh 
  # (enter the busybox pod)
  # wget 172.17.0.14:80
  # (index.html created)
  # (or execute this command to display the result) wget -O- 172.17.0.14:80
  # exit 
  # (busybox pod will be deleted automatically)
# kubectl logs mypod -n ckad-prep 
# kubectl delete pod mypod -n ckad-prep --grace-period=0 --force
# kubectl delete namespace ckad-prep 


--Centralized Configuration Data 
-Creating ConfigMap (Imperative)
(Literal values)
# kubectl create configmap db-config Â¥
  --from-literal=db=staging 
  --from-literal=username=jdoe

(Single file with environment variables)
# kubectl create configmap db-config --from-env-file=config.env 

(File or directory)
# kubectl create configmap db-config Â¥
  --from-file=config.txt
  --from-file=config-data.txt

-Creating ConfigMap (Declarative)
apiVersion: v1 
kind: ConfigMap 
metadata: 
  name: db-config 
data: 
  db: staging 
  username: jdoe 


# Exercise 2
In this exercise, you will first create a ConfigMap from predefined values in a file. Later, you'll create a Pod, consume the ConfigMap as environment variables and print out its values from within the container.

## Configuring a Pod to Use a ConfigMap
1. Create a new file named `config.txt` with the following environment variables as key/value pairs on each line.
- `DB_URL` equates to `localhost:3306`
- `DB_USERNAME` equates to `postgres`
2. Create a new ConfigMap named `db-config` from that file.
3. Create a Pod named `backend` that uses the environment variables from the ConfigMap and runs the container with the image `nginx`.
4. Shell into the Pod and print out the created environment variables. You should find `DB_URL` and `DB_USERNAME` with their appropriate values.
5. (Optional) Discuss: How would you approach hot reloading of values defined by a ConfigMap consumed by an application running in Pod?01234567:02-creating-using-configmap fahmi$

# Solution 2
# vim config.txt 
  DB_URL=localhost:3306
  DB_USERNAME=postgres 
# kubectl create configmap db-config --from-env-file=config.txt
# kubectl run backend --image=nginx --restart=Never -o yaml --dry-run > pod.yaml 
# vi pod.yaml 
  ...
  spec: 
    containers: 
    - image: nginx 
      name: backend 
      envFrom: 
        - configMapRef: 
            name: db-config 
  ...
# kubectl create -f pod.yaml 
# kubectl get pods | grep backend 
# kubectl exec -it backend -- env | grep DB_


--Creating Secrets(Imperative)
# kubectl create secret generic db-creds Â¥
  --from-literal=pwd=s3cre!
# kubectl create secret generic db-creds Â¥
  --from-env-file=secret.env 
# kubectl create secret generic db-creds Â¥
  --from-file=username.txt 
  --from-file=password.txt 

-Value has to be base64-encoded manually 
# echo -n 's3cre!' | base64 
czNjcmUh=

apiVersion: v1 
kind: Secret 
metadata: 
  name: mysecret 
type: Opaque 
data: 
  pwd: czNjcmUh=

--Using Secret as Files from a Pod
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:               # secret in pod 
    - name: foo                 # secret in pod 
      mountPath: "/etc/foo"     # secret in pod 
      readOnly: true            # secret in pod 
  volumes:                      # secret in pod 
  - name: foo                   # secret in pod 
    secret:                     # secret in pod 
      secretName: mysecret      # secret in pod 

--Using Secret as Environment Variables 
apiVersion: v1 
kind: Pod 
metadata: 
  name: mypod 
spec: 
  containers: 
  - name: mycontainer
    image: redis 
    env: 
      - name: SECRET_USERNAME
        valueFrom: 
          secretKeyRef: 
            name: mysecret 
            key: username 
      - name: SECRET_PASSWORD
        valueFrom: 
          secretKeyRef: 
            name: mysecret
            key: password 
  restartPolicy: Never
    


# Exercise 3
In this exercise, you will first create a Secret from literal values. Next, you'll create a Pod and consume the Secret as environment variables. Finally, you'll print out its values from within the container.

## Configuring a Pod to Use a Secret
1. Create a new Secret named `db-credentials` with the key/value pair `db-password=passwd`.
2. Create a Pod named `backend` that defines uses the Secret as environment variable named `DB_PASSWORD` and runs the container with the image `nginx`.
3. Shell into the Pod and print out the created environment variables. You should find `DB_PASSWORD` variable.
4. (Optional) Discuss: What is one of the benefit of using a Secret over a ConfigMap?

# Solution 3 
# kubectl create secret generic db-credentials --from-literal=db-password=passwd
# kubectl get secrets 
# kubectl run backend --image=nginx --restart=Never -o yaml --dry-run > backend-pod.yaml 
# vim backend-pod.yaml 
...
spec: 
  containers: 
  - name: backend 
    image: nginx 
    env:
      - name: DB_PASSWORD 
        valueFrom:
          secretKeyRef:
            name: db-credentials
            key: db-password
...

# kubectl create -f backend-pod.yaml
# kubectl get pod 
# kubectl exec -it backend -- /bin/sh 
  # env | grep DB_PASSWORD


--Security Context 
-Set Security Context for a Pod 
apiVersion: v1
kind: Pod 
metadata: 
  name: security-pod-demo 
spec: 
  securityContext:      #securityContext for Pod  
    runAsUser: 1000     #securityContext for Pod 
    runAsGroup: 3000    #securityContext for Pod 
    fsGroup: 2000       #securityContext for Pod 
  volumes:              #securityContext for Pod 
  - name: sec-ctx-vol   #securityContext for Pod 
    emptyDir: {}        #securityContext for Pod 
  containers: 
  - name: security-container-demo 
    image: busybox 
    volumeMounts:           #securityContext for Container 
    - name: sec-ctx-vol     #securityContext for Container
      mountPath: /data/demo #securityContext for Container

-runAsUser field specifies that for any Containers in the Pod, all processes run with user ID 1000.
-runAsGroup field specifies the primary group ID of 3000 for all processes within any containers of the Pod.
-fsGroup field specified all processes of the container are also part of the supplementary group ID 2000. The owner for volume /data/demo and any files created in that volume will be Group ID 2000.


# Exercise 4
In this exercise, you will create a Pod that defines a filesystem group ID as security context. Based on this security context, you'll create a new file and inspect the outcome of the file creation based on the rule defined earlier.

## Creating a Security Context for a Pod
1. Create a Pod named `secured` that uses the image `nginx` for a single container. Mount an `emptyDir` volume to the directory `/data/app`.
2. Files created on the volume should use the filesystem group ID 3000.
3. Get a shell to the running container and create a new file named `logs.txt` in the directory `/data/app`. List the contents of the directory and write them down.

# Solution 4
# kubectl run secured --image=nginx --restart=Never -o yaml --dry-run > secured-pod.yaml 
# vi secured-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: secured
  name: secured
spec:
  containers:
  - image: nginx
    name: secured
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
------
...
spec: 
  securityContext: 
    fsGroup: 3000
  volumes: 
  - name: sec-ctx-vol 
    emptyDir: {}
  containers: 
  - image: nginx 
    name: secured 
    volumeMounts: 
    - name: sec-ctx-vol 
      mountPath: /data/app 
...

# kubectl create -f secured-pod.yaml 
# kubectl exec -it secured -- /bin/sh 
  # cd /data/app
  # touch logs.txt 
  # ls -l 
  total 0
  -rw-r--r-- 1 root 3000 0 Dec  2 21:34 log.txt
  # exit 


--Resource Boundaries


# Exercise 5
In this exercise, you will create a ResourceQuota with specific CPU and memory limits for a new namespace. Pods created in the namespace will have to adhere to those limits.
## Defining a Podâ€™s Resource Requirements
Create a resource quota named `apps` under the namespace `rq-demo` using the following YAML definition in the file `rq.yaml`.

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: app
spec:
  hard:
    pods: "2"
    requests.cpu: "2"
    requests.memory: 500m
```

1. Create a new Pod that exceeds the limits of the resource quota requirements e.g. by defining 1G of memory. Write down the error message.
2. Change the request limits to fulfill the requirements to ensure that the Pod could be created successfully. Write down the output of the command that renders the used amount of resources for the namespace.

# Solution 5 
# kubectl create namespace rq-demo 
# vi rq.yaml 
apiVersion: v1 
kind: ResourceQuota 
metadata: 
  name: apps
spec: 
  hard: 
    pods: "2"
    requests.cpu: "2"
    requests.memory: 500m

# kubectl create -f rq.yaml -n rq-demo 
# kubectl describe quota -n rq-demo  
# kubectl run test-pod --image=nginx --restart=Never -o yaml > test-pod.yaml
# vi test-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test-pod
  name: test-pod
spec:
  containers:
  - image: nginx
    name: test-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
-----(edit to this one:)
...
spec: 
  containers: 
  - image: nginx 
    name: test-pod 
    resources: 
      requests: 
        memory: "1G"
        cpu: "200m"
-----

# kubectl create -f pod.yaml -n rq-demo 
Error from server (Forbidden): error when creating "test-pod.yaml": pods "test-pod" is forbidden: exceeded quota: apps, requested: requests.cpu=1G, used: requests.cpu=0, limited: requests.cpu=2
# vi test-pod.yaml 
...
spec: 
  resources: 
    requests: 
      memory: "200m"
      cpu: "200m"
...

# kubectl get pods -n rq-demo 
# kubectl create -f pod.yaml -n rq-demo 
# kubectl describe quota --namespace=rq-demo 
Name:            apps
Namespace:       rq-demo
Resource         Used  Hard
--------         ----  ----
pods             1     2
requests.cpu     200m  2
requests.memory  200m  200m


--Service Account
When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default).
When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace.

# Exercise 6
In this exercise, you will create a ServiceAccount and assign it to a Pod.
## Using a ServiceAccount
1. Create a new service account named `backend-team`.
2. Print out the token for the service account in YAML format.
3. Create a Pod named `backend` that uses the image `nginx` and the identity `backend-team` for running processes.
4. Get a shell to the running container and print out the token of the service account.

# Solution 6 
# kubectl create serviceaccount backend-team 
# kubectl get serviceaccount backend-team -o yaml --export 
â†’ --export # Get a resource's YAML without cluster specific information
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: null
  name: backend-team
  selfLink: /api/v1/namespaces/default/serviceaccounts/backend-team
secrets:
- name: backend-team-token-7qxd8

# kubectl run backend --image=nginx --restart=Never -o yaml --dry-run > backend-pod.yaml 
# vi backend-pod.yaml 
...
spec: 
  serviceAccountName: backend-team
...
# kubectl create -f backend-pod.yaml 
# kubectl get pod backend -o yaml | less 
  â†’ find the serviceaccount's secret name: backend-team-token-7qxd8
  â†’ it should be somewhere here: 
  volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: backend-team-token-7qxd8
      readOnly: true
# kubectl exec -it backend -- /bin/sh 
  # ls /var/run/secrets/kubernetes.io/serviceaccount
  # cat token

--or--
# kubectl run backend2 --image=nginx --restart=Never --serviceaccount=backend-team 
# kubectl get pod 
# kubectl exec -it backend2 -- /bin/sh 
  # cd /var/run/secrets/kubernetes.io/serviceaccount
  # ls -l 
  # cat token 









QUESTIONS: 
-How to expose service to external IP by LoadBalancer


https://access.redhat.com/documentation/en-us/openshift_container_platform/4.1/html/logging/configuring-your-cluster-logging-deployment#efk-logging-elasticsearch-limits_efk-logging-elasticsearch

https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/logging






